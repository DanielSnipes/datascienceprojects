---
title: "Database Marketing"
author: "David Cortes"
date: "August 05, 2015"
output:
  html_document:
    toc: true 
---

# Database Marketing

This example is about analyzing a database of customers in a phone company that works by billing monthly fees to customers who suscribe to the company's services. The purpose of the analysis is to able able to determine how long will a customer last at the moment that she suscribes, and to evaluate which sales segments and which customer demographics are more profitable, using survival analysis. Then, these data will be used to create a model that predicts the expected reveneues generated by customers at a horizon of 3 years, for new customers, and of 1 year, for both new and existing customers, according to which city they live in and which sales channel did they come from; by estimating how long tey will stay and how much will they spend.

This has many uses, among others:

* Determining how much money should be allocated to each sales channel and customer segment to try to capture new customers.
* Determining which segments are more profitable and establishing or changing the target market.
* Determining which sales channels, stores and cities are more profitable.
* Estimating expected revenues (with some modifications).
* Setting goals for employers in parts of the post-sales business processes.

The data was taken from the book ["Data Analysis using SQL and Excel" (G. Linoff, 2008)](http://www.amazon.com/Data-Analysis-Using-SQL-Excel/dp/0470099518) and can be found [here](ftp://ftp.data-miners.com/pub/outgoing/textfiles.zip). This exercise was inspired by a chapter from that book. Nothing is known about the company, except that it operates in the US, has a fairly large base of customers and has been in business for decades. The data are anonymized, such as by having city names replaced by "Metropolis" or "Gottham".

## Business Clues

For this data, there are some business clues that will help and drive the analysis:

* Payment is on a monthly basis, so attrition rates are higher at the end of each month.
* Contracts last one year, so a lot of customers leave after the expiry of their initial contract, exactly one year after becoming customers.
* There are different plans with different fees, but customers can freely change from one to another.

## Information in the data

This data has information on:

* Date the customers signed up for the service.
* Sales channel through which they arrived.
* Geographic area where they live.
* For the customers who leaved, there are some categories of reasons why they leaved, such as for not paying or for switching to anoter plan within this same company. These will later be counted as censored, as unfortunately there is no identificator to track these customers on their new plan, as the ID is recreated again after they switch.
* Plan that they have and the fee that they pay.

Additionally, the dataset suffers from left truncation because the company only started to store this information in a database starting from a certain date, so for dates prior to that, there only is information on the customers who haven't quit.


## 1. Building the Tenure model

### 1.1 Loading and cleaning the data

```{r,cache=TRUE}
subs <- read.delim("subs.txt")
subs$start_date=as.Date(subs$start_date,'%Y-%m-%d')
subs$stop_date=as.Date(subs$stop_date,'%Y-%m-%d')
#Records without a start date are not useful
subs=subs[!is.na(subs$start_date),]
#Those with a start date greater than a stop date are misleading
subs=subs[-subs$tenure<0,]
head(subs$start_date[order(subs$start_date)])
#There is a suspicious outlier which is most likely a data entry mistake, so I'll delete it
subs=subs[-which.min(subs$start_date),]
````

### 1.2 Exploring the data

```{r,mesage=FALSE,cache=TRUE,warning=FALSE,cache=TRUE}
library(ggplot2)
library(scales)
suppressPackageStartupMessages(library(dplyr))
pl=ggplot(subs,aes(start_date))+geom_histogram(fill='navy',color='black')+xlab('Customer Start Date')+ylab('Frequency')+theme_bw()+scale_y_continuous(labels = comma)
suppressMessages(print(pl))
pl=ggplot(subs,aes(stop_date))+geom_histogram(fill='navy',color='black')+xlab('Customer Stop Date')+ylab('Frequency')+theme_bw()+scale_y_continuous(labels = comma)
suppressMessages(print(pl))
min(subs$stop_date,na.rm=TRUE)
max(subs$stop_date,na.rm=TRUE)
dim(subs)
rts=subs %>% group_by(market,rate_plan) %>% summarise(n=n()) %>% mutate(prop=n/sum(n)) %>% mutate(pos=cumsum(prop)-prop/2)
ggplot(rts,aes(x='',y=prop,fill=rate_plan))+geom_bar(stat='identity')+coord_polar('y')+facet_grid(. ~ market)+geom_text(aes(x='',y=pos,label=percent(prop)),size=4)+ggtitle("Subscribed plans in each city")+xlab('')+theme(legend.title=element_blank(),legend.position='bottom')+xlab('')+ylab('')

rts=subs %>% group_by(channel,rate_plan) %>% summarise(n=n()) %>% mutate(prop=n/sum(n)) %>% mutate(pos=cumsum(prop)-prop/2)
ggplot(rts,aes(x='',y=prop,fill=rate_plan))+geom_bar(stat='identity')+coord_polar('y')+facet_grid(. ~ channel)+geom_text(aes(x='',y=pos,label=percent(prop)),size=4)+ggtitle("Subscribed plans from each sales channel")+xlab('')+theme(legend.title=element_blank(),legend.position='bottom')+xlab('')+ylab('')

mkts=subs %>% filter(start_date>='2004-01-01') %>% mutate(yr=format(start_date,'%Y')) %>% group_by(yr,market) %>% summarise(n=n()) %>% mutate(prop=n/sum(n)) %>% mutate(pos=cumsum(prop)-prop/2)
ggplot(mkts,aes(x='',y=prop,fill=market))+geom_bar(stat='identity')+coord_polar('y')+facet_grid(. ~ yr)+geom_text(aes(x='',y=pos,label=percent(prop)),size=4)+ggtitle("Procedence of new Customers per year")+xlab('')+theme(legend.title=element_blank(),legend.position='bottom')+xlab('')+ylab('')

mkts=subs %>% filter(start_date>='2004-01-01') %>% mutate(yr=format(start_date,'%Y')) %>% group_by(yr,channel) %>% summarise(n=n()) %>% mutate(prop=n/sum(n)) %>% mutate(pos=cumsum(prop)-prop/2)
ggplot(mkts,aes(x='',y=prop,fill=channel))+geom_bar(stat='identity')+coord_polar('y')+facet_grid(. ~ yr)+geom_text(aes(x='',y=pos,label=percent(prop)),size=4)+ggtitle("New customers per channel, by year")+xlab('')+theme(legend.title=element_blank(),legend.position='bottom')+xlab('')+ylab('')

mkts=subs %>% filter(start_date>='2004-01-01') %>% group_by(market,channel) %>% summarise(n=n()) %>% mutate(prop=n/sum(n)) %>% mutate(pos=cumsum(prop)-prop/2)
ggplot(mkts,aes(x='',y=prop,fill=channel))+geom_bar(stat='identity')+coord_polar('y')+facet_grid(. ~ market)+geom_text(aes(x='',y=pos,label=percent(prop)),size=4)+ggtitle("Proportion of new customers per channel")+xlab('')+theme(legend.title=element_blank(),legend.position='bottom')+xlab('')+ylab('')

```

As can be seen, there are customer subscriptions recorded from as far back as 1988, but customers leaving are recorded only since 2004. The reason is, as previously mentioned, that the company only started recording this information in 01/01/2004, so the data are left truncated; and also right censored, since it stops in 12/28/2006 (having active customers). The usual approach in these cases is to discard these data, although estimates of survivals (remaining customers) and hazard rates (customers who leave) can be obtained by using time interval estimation.

However, this approach would assume that these rates/probabilities at different customer tenure times don't change in time, which is not a very reasonable asusmption, so I'll analyse the data by simply discarding the customers who suscribed before 01/01/2004, to get the more contemporary numbers, with the disadvantage that these numbers only describe 3 years of data.

From these descriptive plots, it can also be seen that:
* The proportion of customers getting each plan is the same across different cities and from different sales channels.
* The proportion of new customers from each city has been fairly stable in these three years.
* The share of mail sales as a proportion of total sales has been increasing, while the other channels have been fairly proportional.
* There is a disproportionately large number of customers from Gotham who come from the channel 'Dealer', and a disproportionately small number who come from 'Chain'.
* The Gotham market seems to be the most different to the other two.


### 1.3 Transforming the data

Most of this analysis will be done by analyzing survival and hazard curves. In typical survival analysis, these represent the proportion of subjects/animals who are alive after X time and the rate at which they die at X time, but in this case 'survival' means retained customers X time after subscribing and the hazard rate is the proportion of customers who leave after X time from subscribing.

```{r,cache=TRUE,message=FALSE}
library(survival)
library(reshape2)
subs2=subs
subs2=subs2[subs2$start_date>='2004-01-01',]
#There is fortunately already a variable indicating tenure time in days, so there's no need to recalculate it.
#There's also a column indicating which customers were active at the moment the data was taken - in other words, right censored.
#Here 'M' designates the customers who moved to another plan, so I'll take them as censored as they cannot be tracked onto their new plan
subs2$censored[subs2$stop_type=='M']=1
subs2$event=1-1*(subs2$censored==1)
s=Surv(subs2$tenure,subs2$event,type='right')
s1=survfit(s~1)
s2=survfit(s~subs2$market)
s3=survfit(s~subs2$channel)
s3.5=survfit(s~subs2$market+subs2$channel)
s4=survfit(s~format(subs2$start_date,'%Y'))
s4.5=survfit(s~format(subs2$start_date,'%Y')+subs2$channel)
s4.7=survfit(s~format(subs2$start_date,'%Y')+subs2$market)
s5=survfit(s~subs2$rate_plan)
#This is the retention curve, which is similar to the survival curve
suppressPackageStartupMessages(library(dplyr))
s6=subs2 %>% group_by(start_date) %>% summarise(active=mean(censored)) %>% arrange(desc(start_date))
s6$time=1:dim(s6)[1]
s6$surv=s1$surv
s6$start_date=NULL
s6=melt(s6, id.vars='time')
names(s6)=c('time','curve','value')
levels(s6$curve)=c('Retention','Survival')
```
```{r,cache=TRUE}
#Some transformations are not natively supported by R's survival analysis libraries
s15=as.data.frame(matrix(nrow=1092,ncol=1))
names(s15)=c('time')
s15$time=1:1092
temp=subs2 %>% group_by(tenure) %>% summarise(cnt=n()) %>% arrange(desc(tenure)) %>% select(tenure,cnt) %>% mutate(risk=cumsum(cnt)) %>% arrange(tenure) %>% select(tenure,risk)
s15=s15 %>% left_join(temp,by=c('time'='tenure'))
temp=subs2 %>% filter(stop_type==levels(subs2$stop_type)[1]) %>% group_by(tenure) %>% summarise(c=n()) %>% arrange(tenure) %>% select(tenure,c)
s15=s15 %>% left_join(temp,by=c('time'='tenure'))
temp=subs2 %>% filter(stop_type=='V') %>% group_by(tenure) %>% summarise(v=n()) %>% arrange(tenure) %>% select(tenure,v)
s15=s15 %>% left_join(temp,by=c('time'='tenure'))
temp=subs2 %>% filter(stop_type=='M') %>% group_by(tenure) %>% summarise(m=n()) %>% arrange(tenure) %>% select(tenure,m)
s15=s15 %>% left_join(temp,by=c('time'='tenure'))
temp=subs2 %>% filter(stop_type=='I') %>% group_by(tenure) %>% summarise(i=n()) %>% arrange(tenure) %>% select(tenure,i)
s15=s15 %>% left_join(temp,by=c('time'='tenure'))
for (i in names(s15)){
  for (j in which(is.na(s15[,i]))){
    s15[j,i]=0
  }
}

s15$h=(s15$v+s15$m+s15$i)/(s15$risk-s15$c)
s15$h[is.na(s15$h)]=0
s15$s=cumprod(1-s15$h)
s15$v2=(cumsum(s15$v)/(cumsum(s15$v)+cumsum(s15$m)+cumsum(s15$i)))*(1-s15$s)
s15$i2=(cumsum(s15$i)/(cumsum(s15$v)+cumsum(s15$m)+cumsum(s15$i)))*(1-s15$s)
s15$m2=(cumsum(s15$m)/(cumsum(s15$v)+cumsum(s15$m)+cumsum(s15$i)))*(1-s15$s)

s15=s15[,c(1,8:11)]
names(s15)[2:5]=c('Active','Leaved voluntarily','Did not pay','Changed Plan')
library(reshape2)
s15=melt(s15,id.vars='time')
```


### 1.4 Visualizing customer retention and attrition

*Note: this would be better and easier done with Tableau, but since I'm using R+knitr showing the computations, I'll do the whole analysis from R only.*

```{r,message=FALSE,cache=TRUE}
library(ggplot2)
library(ggthemes)
library(scales)
library(gridExtra)
library(reshape2)
suppressPackageStartupMessages(library(dplyr))

pl11=data.frame(s1$time,s1$`n.event`/s1$`n.risk`)
pl1=data.frame(s1$time,s1$surv)

pl1=ggplot(pl1,aes(s1.time,s1.surv))+geom_line(color='navy')+ggtitle('Empirical Survival Curve')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,1))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()

pl11=ggplot(pl11,aes(s1.time,s1.n.event.s1.n.risk))+geom_line(color='darkred')+ggtitle('Empirical Hazard Rate')+ylab('Customers leaving')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,.025))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()

pl15=ggplot(s15,aes(time,value,fill=variable,group=variable))+theme_bw()+geom_area(position="fill")+scale_y_continuous(labels=percent)+ylab('Proportion of Customers')+theme(legend.title=element_blank(),legend.position='top')+ggtitle("Situation of Customers")+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+xlab('Months After Subscribing')

pl21=data.frame(s2$time,s2$`n.event`/s2$`n.risk`,c(rep('Gotham',1092),rep('Metropolis',1092),rep('Smallville',1092)))
names(pl21)=c('time','hazard','market')
pl2=data.frame(s2$time,s2$surv,c(rep('Gotham',1092),rep('Metropolis',1092),rep('Smallville',1092)))
names(pl2)=c('time','hazard','market')

pl21=ggplot(pl21,aes(time,hazard,group=market,color=market))+geom_line()+ggtitle('Empirical Hazard Rate')+ylab('Customers leaving')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,.035))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()+scale_colour_stata()

pl2=ggplot(pl2,aes(time,hazard,group=market,color=market))+geom_line()+ggtitle('Empirical Survival Curve')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,1))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()+scale_colour_tableau()


pl31=data.frame(s3$time,s3$`n.event`/s3$`n.risk`,c(rep('Chain',1092),rep('Dealer',1092),rep('Mail',1092),rep('Store',1092)))
names(pl31)=c('time','hazard','channel')
pl3=data.frame(s3$time,s3$surv,c(rep('Chain',1092),rep('Dealer',1092),rep('Mail',1092),rep('Store',1092)))
names(pl3)=c('time','hazard','channel')
pl31=ggplot(pl31,aes(time,hazard,group=channel,color=channel))+geom_line()+ggtitle('Empirical Hazard Rate')+ylab('Customers leaving')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,.035))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()+scale_colour_stata()

pl3=ggplot(pl3,aes(time,hazard,group=channel,color=channel))+geom_line()+ggtitle('Empirical Survival Curve')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_y_continuous(labels=percent,limits=c(0,1))+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()+scale_colour_tableau()


pl35=data.frame(s3.5$time,s3.5$surv,c(rep('Gotham',1092*4),rep('Metropolis',(1092*2+1091*2)),rep('Smallville',(1091+1092+1090+1091))),
                c(rep('Chain',1092),rep('Dealer',1092),rep('Mail',1092),rep('Store',1092),
                rep('Chain',1092),rep('Dealer',1092),rep('Mail',1091),rep('Store',1091),
                rep('Chain',1091),rep('Dealer',1092),rep('Mail',1090),rep('Store',1091)))
names(pl35)=c('time','survival','market','channel')

pl35=ggplot(pl35,aes(time,survival))+geom_line(color='darkred')+facet_grid(channel~market)+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+theme_bw()+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Empirical Survival Curve by Market and Channel')+ylab('Remaining Customers')+xlab('Months after suscribing')


pl4=data.frame(s4$time,s4$surv,c(rep('2004',1092),rep('2005',726),rep('2006',361)))
names(pl4)=c('time','survival','year')
pl4=ggplot(pl4,aes(time,survival,group=year,color=year))+geom_line()+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Empirical Survival Curve by Year of Subscription')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_colour_hc()+theme(legend.position='top')

pl45=data.frame(s4.5$time,s4.5$surv,c(rep('2004',1092*4),rep('2005',726*4),rep('2006',361*4)),
                c(rep('Chain',1092),rep('Dealer',1092),rep('Mail',1092),rep('Store',1092),
                  rep('Chain',726),rep('Dealer',726),rep('Mail',726),rep('Store',726),
                  rep('Chain',361),rep('Dealer',361),rep('Mail',361),rep('Store',361)))
names(pl45)=c('time','survival','year','channel')

pl45=ggplot(pl45,aes(time,survival,group=year,color=year))+geom_line()+facet_grid(channel~.)+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Empirical Survival Curve by Year of Subscription, per Channel')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_colour_wsj()+theme(legend.position='top')

pl47=data.frame(s4.7$time,s4.7$surv,c(rep('2004',1092*3),rep('2005',726*3),rep('2006',361*3)),
                c(rep('Gotham',1092),rep('Metropolis',1092),rep('Smallville',1092),
                  rep('Gotham',726),rep('Metropolis',726),rep('Smallville',726),
                  rep('Gotham',361),rep('Metropolis',361),rep('Smallville',361)))
names(pl47)=c('time','survival','year','market')
pl47=ggplot(pl47,aes(time,survival,group=year,color=year))+geom_line()+facet_grid(market~.)+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Empirical Survival Curve by Year of Subscription, per Market')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_colour_wsj()+theme(legend.position='top')

pl5=data.frame(s5$time,s5$surv,c(rep('Bottom',1092),rep('Middle',1092),rep('Top',1092)))
names(pl5)=c('time','survival','plan')
pl5=ggplot(pl5,aes(time,survival,group=plan,color=plan))+geom_line()+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Empirical Survival Curve by Year of Subscription, per Plan')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_colour_solarized()+theme(legend.position='top')

pl6=ggplot(s6,aes(time,value,group=curve,color=curve))+geom_line()+geom_line(data=subset(s6,curve=='Survival'),size=1.5)+geom_smooth(se=FALSE,data=subset(s6,curve=='Retention'),size=2)+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ggtitle('Survival vs. Retention')+ylab('Remaining Customers')+xlab('Months after suscribing')+scale_colour_wsj()+theme(legend.position='top')

grid.arrange(pl1,pl11,nrow=2,ncol=1);print(pl15);grid.arrange(pl2,pl21,nrow=2,ncol=1);grid.arrange(pl3,pl31,nrow=2,ncol=1);print(pl35);print(pl4);print(pl45);print(pl47);print(pl5);print(pl6)
```

### 1.5 Insights gained from it

From the plots on the previous section, it can be seen that:

* The attrition rate is high during the first month, then it decreases and spikes a lot exactly at one year, which coincides with the date on which the contracts expire. There is again this same spike at the end of the second and thir years, but it's shrinking by a lot each year.
* Other than the end-of-year spikes, the attrition rate seems to be fairly constant in time.
* Most of the customers leave voluntarily, a verly small fraction change plans and a significant proportion are shut because of no-payment. The company should aim to capture less non-payers.
* Smallville has the best retention curve and Gotham the worst. The difference among these three markets is huge. Gotham's clients seem to be particularly susceptible to the end-of-year attrition, which is to be expected since the mix of customers from Gotham is very different than that from the other two markets, having mostly customers from the dealer channel, which is also the channel most affected by end-of-year attrition.
* Customers who come from the store channel are the most loyal. Customers who come from the dealer channel are the second most loyal up to the end of the first year, and after that they turn out to be the most unloyal.
* Some segments are far more susceptible to end-of-year attrition than others. Customers from Smallville seem to be more loyal regardless of the channel they come from.
* Store is the best-performing channel in every market, whereas the worst-performing one varies by market.
* Overall retention rates seem to have been a lot from 2004 to 2005 - especially during the first year - but then decreased again in 2006. Another plot reveals that this increase was driven from the chain channel only. However, the retention rates seem to have universally decrease for all markets and channels from 2005 to 2006.
* In Smallville - the most attractive market for this company - retention rates have been significantly decreasing in the last 3 years.
* Customers from the most expensive plans are the less loyal. Customers from the cheapest plans are the most affected by end-of-year attrition.
* In the last plot (the wide red line is a smoothed version of the narrow one), points at which the red line is over the blue line indicate periods (in terms of months back in time from the current date, in that case 28/12/2006 since that's when the data ends) in which the company has been performing better than its historic average, whereas period where the red curve is below the blue one signal periods in which the company has been performing worse than its historic average.
* Customers who already passed their first-year anniversary are a lot more valuable than customers with less than one year, since a very high proportion is expected to leave around the one-year anniversary. Longer-tenured customers in general seem to be more loyal, as is to be expected.



### 1.6 Visualizations vs. Statistics

Normally, parametric models of survival analysis are used to estimate survival and hazard functions over time and in theory it could be possible to build a model to predict the rest of the survival curve for longer time periods, but in this case the hazard function looks nothing similar to any of the survival models that exist out there. One would initially think of treating empirically-derived survival curvives as a univariate time series to predict survival rates for longer tenures from time-series or machine-learning models.

However, as illustrated in the book ["Data Mining Techniques" (G. Linoff & J. Berry, 2010)](http://www.amazon.com/Data-Mining-Techniques-Relationship-Management/dp/0470650931), this oftentimes leads to misleading results for survival analysis applied to customer data, because these functions tend to fit very well the data for the time period that they are fed, but tend to provide poor predictions for future times.

Thus, I'll illustrate here how different models constructed with the aggregated survival curve from the first plot considering only the first one-and-a-half and/or two years would perform on the real data:

```{r,message=FALSE,warning=FALSE,cache=TRUE}
library(caret)
library(kernlab)
library(quantmod)
library(forecast)
library(reshape2)
library(ggplot2)
library(ggthemes)
library(scales)
whole=data.frame(s1$time,s1$surv)
names(whole)=c('time','surv')
whole[,3:7]=Lag(whole$surv,k=1:5)
names(whole)[3:7]=c('lag1','lag2','lag3','lag4','lag5')


train=whole[6:730,]
test=whole[731:1092,]

train2=ts(whole$surv[1:730],start=1,frequency=365)

#Polynomial Regression
pr=lm(surv~poly(time,3)+poly(lag1,3)+poly(lag2,3)+poly(lag3,3)+poly(lag4,3)+poly(lag5,3),data=train)
pr.pred=rep(NA,1092)
for (i in 726:730){
  pr.pred[i]=whole$surv[i]
}
for (i in 731:1092){
  vals=data.frame(cbind(i,pr.pred[i-1],pr.pred[i-2],pr.pred[i-3],pr.pred[i-4],pr.pred[i-5]))
  names(vals)=c('time','lag1','lag2','lag3','lag4','lag5')
  if (is.na(vals$lag1)|is.infinite(vals$lag1)|abs(vals$lag1)>=.43){
    pr.pred[i]=NA
  } else {
      pr.pred[i]=predict(pr,vals)
  }
}
for (i in 726:730){
  pr.pred[i]=NA
}

#Support Vector Regression with Gaussian Kernel
svr=train(surv~.,data=train,method='svmRadial',tuneLength=14,preProc=c('center','scale'),trControl=trainControl(method='cv'))
svr.pred=rep(NA,1092)
for (i in 726:730){
  svr.pred[i]=whole$surv[i]
}
for (i in 731:1092){
  vals=data.frame(cbind(i,svr.pred[i-1],svr.pred[i-2],svr.pred[i-3],svr.pred[i-4],svr.pred[i-5]))
  names(vals)=c('time','lag1','lag2','lag3','lag4','lag5')
  if (is.na(vals$lag1)|is.infinite(vals$lag1)|abs(vals$lag1)>=.5){
    svr.pred[i]=NA
  } else {
      svr.pred[i]=predict(svr,vals)
  }
}
for (i in 726:730){
  svr.pred[i]=NA
}

#Holtz-Winter's Exponential Smoothing
hw=HoltWinters(train2)
hw.pred=c(forecast(hw,(1092-730))$x,forecast(hw,(1092-730))$mean)

#Auto-Arima
ar=auto.arima(train2)
ar.pred=c(forecast(ar,(1092-730))$x,forecast(ar,(1092-730))$mean)

#Predicting the 3rd year
fc=data.frame(whole$time,whole$surv,c(rep(NA,730),ar.pred[731:1092]),c(rep(NA,730),hw.pred[731:1092]),svr.pred,pr.pred)
names(fc)=c('time','real','auto-arima','holtz-winters','support vector regression','polynomial regression')

fc=melt(fc,id.vars='time')
names(fc)[2:3]=c('model','value')

ggplot(fc,aes(time,value,group=model,color=model))+geom_line(data=subset(fc,model=='real'),size=1.7)+geom_line(size=1.1,data=subset(fc,model!='real'))+ggtitle('Forecasted Survival rate in the 3rd year')+theme_bw()+scale_x_continuous(breaks=seq(0,(3*12),3)*(365/12),labels=seq(0,(3*12),3))+scale_y_continuous(labels=percent,limits=c(0,1))+ylab('Proportion of Remaining Customers')+xlab('Months after suscribing')+scale_colour_wsj()+theme(legend.position='top')+geom_vline(xintercept=(730-5))
```

In this case, the predictions were halted when they started becoming too unstable. In this case, Holz-Winters seemed to provide the best forecast, but still far from correct.


## 2. Building the Expected Revenue models


Having the empiricall hazard rates at different points in time, the expected revenue could be calculated as the sum of payed fee multiplied by estimated remaining proportion of customers at different months. As there is more fine-grained data here, it's possible to use different hazards according to the market where the customer is from and the sales channel through which she arrived. It's easy to build a model for new customers, since there are estimates from up to 3 years since they suscribe, but for existing customers, there's not enough data to be able to do long forecasts if they have already been there for a year or two (keep in mind that the hazard probability decreases a bit with tenure).

For these purposes, I'll assume that customers pay the monthly fee at the end of the month whenever they are 

### 2.1.2 Expected Revenue after 3 years (New Customers Only)

```{r,cache=TRUE}
library(reshape2)
suppressPackageStartupMessages(library(dplyr))
f=data.frame(s3.5$time,s3.5$surv,s3.5$n.event/s3.5$n.risk,c(rep('Gotham',1092*4),rep('Metropolis',(1092*2+1091*2)),rep('Smallville',(1091+1092+1090+1091))),            c(rep('Chain',1092),rep('Dealer',1092),rep('Mail',1092),rep('Store',1092),
                rep('Chain',1092),rep('Dealer',1092),rep('Mail',1091),rep('Store',1091),
                rep('Chain',1091),rep('Dealer',1092),rep('Mail',1090),rep('Store',1091)))
names(f)=c('time','survival','hazard','market','channel')
lookup.surv=dcast(f,time~market+channel,value.var="survival",fill=NA)
for (i in names(lookup.surv)[2:13]){
  for (j in which(is.na(lookup.surv[,i]))){
    lookup.surv[j,i]=lookup.surv[j-1,i]
  }
}
lookup.surv=lookup.surv %>% arrange(time)
forecast_three_years=function(market,channel,rate){
  key=paste(market,channel,sep='_')
  sum(lookup.surv[round(seq(1,1092,365/12),0),key]*rate)
}
```

Now I'll see the estimated revenues for new customers from different markets and segments, supposing that they pay the median monthly fee of $35:

```{r,cache=TRUE}
markets=c("Smallville",'Gotham','Metropolis')
channels=c('Chain','Store','Mail','Dealer')
grid=expand.grid(markets,channels)
grid$rate=35
names(grid)[1:2]=c('market','channel')
grid$forecast=0
for (i in 1:dim(grid)[1]){
  grid$forecast[i]=forecast_three_years(grid$market[i], grid$channel[i], grid$rate[i])
}
print(grid)
```


### 2.1.1 Expected Revenue after 1 year (all customers)

```{r}
forecast_one_year=function(market,channel,tenure,rate){
  key=paste(market,channel,sep='_')
  init=findInterval(tenure,seq(1,1092,365/12))
  div=lookup.surv[init,key]
  sq=round(head(seq(init,1092,365/12),12),0)
  sum((lookup.surv[sq,key]/div)*rate)
}
#Note: tenure here is the time in days since a client has been suscribed, and the maximum value
#for which it can be forecasted is 1092-366=726
```

```{r}
tenures=c(1,90,180,365,700)
grid=expand.grid(markets,channels,tenures)
names(grid)[1:3]=c('market','channel','tenure')
grid$rate=35
grid$forecast=0
suppressPackageStartupMessages(library(dplyr))
grid=grid %>% arrange(market,channel,tenure)
for (i in 1:dim(grid)[1]){
  grid$forecast[i]=forecast_one_year(grid$market[i], grid$channel[i], grid$tenure[i], grid$rate[i])
}
print(grid)
```


## 3. Conclusions


From these data, it was possible to get reliable estimates of expected revenue per customer according to its segment and the fee that they pay (or using the median fee). Forecasting survival curves turned out to be disastrous and so only three years of tenure modeling could be employed for these forecasts, although it would have been nice to build a longer-horizon model with discounted cash flows to get customer lifetime estimates. This kind of customer-centric forecast is oftentimes better than traditional forecasting methods and can also be used for other purposes such as budget allocation. It could be found out which segments provide more loyal customers and which are less reliable, giving some insights as to where to focus the efforts. It was also found out that overall retention has been decreasing in most segments and the company seems not to be targeting the most profitable customers. This analysis would have been served to know where to focus better and what to expect. The company should really look after its Smallville market, which is the most attractive and it's starting to lose.